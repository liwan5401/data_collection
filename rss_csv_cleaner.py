#!/usr/bin/env python3
"""Clean the RSS keyword CSV output by applying publish-date and content filters."""

from __future__ import annotations

import argparse
import csv
import datetime as dt
import logging
from pathlib import Path
from typing import Iterable, Mapping, Optional, Sequence

LOGGER = logging.getLogger("rss_csv_cleaner")

DEFAULT_INPUT = "output/rss_keyword_matches.csv"
DEFAULT_OUTPUT = "output/rss_keyword_matches_cleaned.csv"
DEFAULT_MIN_YEAR = 2024
DEFAULT_MIN_CHARS = 100
DEFAULT_ENCODING = "utf-8"
DEFAULT_FIELD_LIMIT = 10_000_000  # 10 MB per field


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Filter RSS keyword CSV results.")
    parser.add_argument("--input", default=DEFAULT_INPUT, help="Input CSV generated by rss_keyword_scanner.")
    parser.add_argument("--output", default=DEFAULT_OUTPUT, help="Cleaned CSV output path.")
    parser.add_argument("--min-year", type=int, default=DEFAULT_MIN_YEAR, help="Keep entries published in or after this year.")
    parser.add_argument("--min-chars", type=int, default=DEFAULT_MIN_CHARS, help="Minimum characters required in article_text.")
    parser.add_argument("--encoding", default=DEFAULT_ENCODING, help="CSV encoding for both input and output.")
    parser.add_argument(
        "--field-limit",
        type=int,
        default=DEFAULT_FIELD_LIMIT,
        help="Maximum CSV field size in bytes (default: %(default)s).",
    )
    parser.add_argument("--log-level", default="INFO", help="Logging level (default: INFO).")
    return parser.parse_args(argv)


def parse_year(published: str) -> Optional[int]:
    published = published.strip()
    if not published:
        return None

    # Try a few common date formats.
    patterns = (
        "%Y-%m-%d",
        "%Y/%m/%d",
        "%d %b %Y",
        "%d %B %Y",
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S %Z",
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%dT%H:%M:%S%z",
    )

    for pattern in patterns:
        try:
            return dt.datetime.strptime(published, pattern).year
        except ValueError:
            continue

    # Fallback: extract the first four-digit year.
    for token in published.split():
        if len(token) == 4 and token.isdigit():
            year = int(token)
            if 1900 <= year <= 2100:
                return year
    return None


def clean_text(value: str) -> str:
    if not value:
        return ""
    cleaned = value.replace("\r", " ").replace("\n", " ").replace("\t", " ")
    cleaned = cleaned.replace("\ufeff", "").replace("\u2028", " ").replace("\u2029", " ")
    return " ".join(cleaned.split())


def filter_rows(
    rows: Iterable[Mapping[str, str]],
    *,
    min_year: int,
    min_chars: int,
) -> Iterable[Mapping[str, str]]:
    for row in rows:
        published = row.get("published", "")
        year = parse_year(published)
        if year is None or year < min_year:
            continue

        text = clean_text(row.get("article_text", ""))
        if len(text) < min_chars:
            continue

        cleaned_row = dict(row)
        cleaned_row["article_text"] = text
        cleaned_row["entry_title"] = clean_text(cleaned_row.get("entry_title", ""))
        cleaned_row["entry_author"] = clean_text(cleaned_row.get("entry_author", ""))
        cleaned_row["categories"] = clean_text(cleaned_row.get("categories", ""))
        cleaned_row["feed_title"] = clean_text(cleaned_row.get("feed_title", ""))
        yield cleaned_row


def safe_row_iterator(reader: csv.DictReader) -> Iterable[Mapping[str, str]]:
    row_index = 0
    while True:
        try:
            row = next(reader)
            row_index += 1
            yield row
        except StopIteration:
            break
        except csv.Error as exc:
            row_index += 1
            LOGGER.warning("Skipping malformed row near line %s: %s", row_index, exc)
            continue


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(message)s",
    )

    try:
        csv.field_size_limit(args.field_limit)
    except (OverflowError, ValueError) as exc:
        LOGGER.warning(
            "Invalid field limit %s (%s). Using default %s.",
            args.field_limit,
            exc,
            DEFAULT_FIELD_LIMIT,
        )
        csv.field_size_limit(DEFAULT_FIELD_LIMIT)

    input_path = Path(args.input).expanduser()
    output_path = Path(args.output).expanduser()

    if not input_path.exists():
        LOGGER.error("Input CSV not found: %s", input_path)
        return 1

    with input_path.open("r", encoding=args.encoding, newline="") as inp, output_path.open(
        "w", encoding=args.encoding, newline=""
    ) as out:
        reader = csv.DictReader(inp)
        if not reader.fieldnames:
            LOGGER.error("Input CSV %s has no header row.", input_path)
            return 1

        writer = csv.DictWriter(out, fieldnames=reader.fieldnames)
        writer.writeheader()

        written = 0
        for row in filter_rows(
            safe_row_iterator(reader),
            min_year=args.min_year,
            min_chars=args.min_chars,
        ):
            writer.writerow(row)
            written += 1

    LOGGER.info("Cleaned CSV written to %s (%s rows).", output_path, written)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
